## Wrangle and Analyze Twitter Data Using Python  

**Background**    
Real-world data rarely comes clean. Using Python and its libraries, you will gather data from a variety of sources and in a variety of formats, assess its quality and tidiness, then clean it. This is called data wrangling. You will document your wrangling efforts in a Jupyter Notebook, plus showcase them through analyses and visualizations using Python (and its libraries) and/or SQL.  

The dataset that you will be wrangling (and analyzing and visualizing) is the tweet archive of Twitter user @dog_rates, also known as WeRateDogs. WeRateDogs is a Twitter account that rates people's dogs with a humorous comment about the dog. These ratings almost always have a denominator of 10. The numerators, though? Almost always greater than 10. 11/10, 12/10, 13/10, etc. Why? Because "they're good dogs Brent." WeRateDogs has over 4 million followers and has received international media coverage.    
 
 **Project Details**  
Your tasks in this project are as follows:  

- Data wrangling, which consists of:  
  - Gathering data  
  - Assessing data  
  - Cleaning data  
- Storing, analyzing, and visualizing your wrangled data  
- Reporting on 1) your data wrangling efforts and 2) your data analyses and visualizations  

 
**Objective:**  Gather data from 3 disparate systems (Excel file, website, and Twitter API). Wrangle and merge the datasets into a master file, then analyze it highlighting at least 3 insights with visualizations. 

**Main project file**: Wrangle and Analyze Data.ipynb  
**Source files**: twitter-archive-enhanced.csv, image-predictions.tsv, tweet_json.txt    
**Created file**: twitter_archive_master.csv  
  
  
### Software and Python Packages   
Jupyter Notebook  
Python (3.6 or higher)  

**Packages**  
- Pandas  
- Numpy  
- json  
- Requests  
- Tweepy  
- re  
- Matplotlib  
  


